{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.janisklaise.com/post/rl-policy-gradients/\n",
    "\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticPolicy:\n",
    "\n",
    "    def __init__(self, teta, alfa, gamma):\n",
    "        # Init params\n",
    "        self.teta = teta # weights?\n",
    "        self.alfa = alfa # learning rate\n",
    "        self.gamma = gamma # discount factor for rwds\n",
    "\n",
    "    def logistic(self, y):\n",
    "        # Logistic/sigmoid function\n",
    "        return 1 / (1 + np.exp(-y))\n",
    "    \n",
    "    def probs(self, x):\n",
    "        # Return the probs between the 2 actions of the pole\n",
    "        # Matrix mult between input and weights\n",
    "        y = np.dot(x, self.teta) # equals to @ op\n",
    "        prob_zero = self.logistic(y)\n",
    "\n",
    "        # Return the probs array for the 2 actions Sum = 1\n",
    "        return np.array([prob_zero, 1-prob_zero])\n",
    "\n",
    "    def act(self, x):\n",
    "        # Sample an action from probs\n",
    "        probs = self.probs(x)\n",
    "        action = np.random.choice([0, 1], p=probs)\n",
    "\n",
    "        # Return the action and the probs of that action\n",
    "        return action, probs[action]\n",
    "\n",
    "    def grad_log_p(self, x):\n",
    "        # Compute grad_log_probs\n",
    "        y = np.dot(x, self.teta)\n",
    "        # TODO: understand these 2 lines\n",
    "        grad_log_p_zero = x - x*self.logistic(y)\n",
    "        grad_log_p_one = - x*self.logistic(y)\n",
    "\n",
    "        return grad_log_p_zero, grad_log_p_one\n",
    "\n",
    "    def grad_log_p_dot_rewards(self, grad_log_p, actions, discounted_rewards):\n",
    "        # dot grads with future rewards for each action in episode\n",
    "        return grad_log_p.T @ discounted_rewards\n",
    "\n",
    "    def discount_rewards(self, grad_log_p, actions, discounted_rewards):\n",
    "        # dot gras with future rewards for each action in episode\n",
    "        # TODO: understand this fnc\n",
    "        return np.dot(grad_log_p.T, discounted_rewards)\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        # compute temporally adjusted, discounted rewards\n",
    "        discounted_rewards = np.zeros(len(rewards))\n",
    "        cumulative_rewards = 0\n",
    "\n",
    "        for i in reversed(range(0, len(rewards))):\n",
    "            cumulative_rewards = cumulative_rewards * self.gamma + rewards[i]\n",
    "            discounted_rewards[i] = cumulative_rewards\n",
    "\n",
    "        return discounted_rewards\n",
    "\n",
    "\n",
    "    def update(self, rewards, obs, actions):\n",
    "        # compute gradients for each action over all obs\n",
    "        grads = [self.grad_log_p(ob)[action] for ob,action in zip(obs,actions)]\n",
    "        grad_log_p = np.array(grads)\n",
    "\n",
    "        # compute temporaly adjusted, discounted rewards\n",
    "        discounted_rewards = self.discount_rewards(rewards)\n",
    "\n",
    "        dot = self.grad_log_p_dot_rewards(grad_log_p, actions, discounted_rewards)\n",
    "\n",
    "        # Gradient ascent\n",
    "        self.teta += self.alfa*dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, render=False):\n",
    "    ob = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    obs = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    probs = []\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        obs.append(ob)\n",
    "\n",
    "        # Get the action to do\n",
    "        action, prob = policy.act(ob)\n",
    "\n",
    "        # get next ob and reward\n",
    "        ob, reward, done, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        probs.append(prob)\n",
    "\n",
    "\n",
    "    return totalreward, np.array(rewards), np.array(observations), np.array(actions), np.array(probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}